{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasources = (\n",
    "        # './datasets/stack_exchange_preferences/train.pkl',  # this demands more GPU RAM because we have more than 2 responses per each sample\n",
    "        './datasets/hh-rlhf/train.pkl',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_id: int, max_seq_len: int, full_pad: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    assert len(batch) == 1, 'This script only support one item at a time to validate RM model'\n",
    "    tokens_list = batch[0]\n",
    "\n",
    "    # Note we assume the tokens are for the ordered completions for a given prompt,\n",
    "    # where the best completion is the first, and worst completion is the last.\n",
    "\n",
    "    max_batch_seq_len = max([len(tokens) for tokens in tokens_list])\n",
    "    assert max_batch_seq_len <= max_seq_len\n",
    "\n",
    "    if full_pad:\n",
    "        max_batch_seq_len = max_seq_len\n",
    "\n",
    "    # concatenate prompt, completion together\n",
    "    batch_size = len(tokens_list)\n",
    "\n",
    "    batch_sequences = torch.full((batch_size, max_batch_seq_len), pad_id, dtype=torch.long)\n",
    "\n",
    "    # record the terminal index of the completion, often referred to as the terminal time step in RL\n",
    "    terminal_steps = torch.zeros((batch_size), dtype=torch.long)\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        seq = torch.tensor(tokens, dtype=torch.long)\n",
    "        seq_len = len(seq)\n",
    "\n",
    "        batch_sequences[i, :seq_len] = seq\n",
    "        terminal_steps[i] = seq_len - 1  # minus 1 because indexing starts from zero\n",
    "\n",
    "    return batch_sequences, terminal_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from instruct_llama.tokenizer import Tokenizer\n",
    "from instruct_llama.configs.rm_lora import config as cfg\n",
    "from instruct_llama.utils.custom_dataset import ComparisonsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\"/media/ivirse/Data1/Project_ISOFH/Materials/NLP/llama/tokenizer.model\")\n",
    "\n",
    "_collate_fn = functools.partial(\n",
    "        custom_collate_fn,\n",
    "        pad_id=tokenizer.eos_id,\n",
    "        max_seq_len=cfg.max_seq_len,\n",
    "        full_pad=cfg.full_pad,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ComparisonsDataset(data_sources=train_datasources, max_seq_len=cfg.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dim = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    vocab_size: int = 32000\n",
    "    \n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_kwargs = {\n",
    "        'num_workers': 0,\n",
    "        'batch_size': 1,  # always work on one sample at a time\n",
    "        'pin_memory': True,\n",
    "        'shuffle': True,\n",
    "        'sampler': None,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, collate_fn=_collate_fn, **cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"flow\n",
    "tokens -> embedding -> attention -> ff\n",
    "[2, tokensize]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.Embedding(32000, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output transformer layer\n",
    "dim = 4096\n",
    "hidden_dim = 4 * dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_head = torch.nn.Linear(4096, 1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = args.dim // args.n_heads\n",
    "n_heads = args.n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq = torch.nn.Linear(args.dim, args.n_heads * head_dim, bias=False)\n",
    "wk = torch.nn.Linear(args.dim, n_heads * head_dim, bias=False)\n",
    "wv = torch.nn.Linear(args.dim, n_heads * head_dim, bias=False)\n",
    "wo = torch.nn.Linear(args.n_heads * head_dim, args.dim, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0288, -0.0127], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-0.0090, -0.0032], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-0.0376, -0.0077], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-0.0314, -0.0275], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-0.0619, -0.0093], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-0.0421, -0.0415], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/ivirse/Data1/Project_ISOFH/Materials/NLP/InstructLLaMA/test.ipynb Cell 18\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ivirse/Data1/Project_ISOFH/Materials/NLP/InstructLLaMA/test.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Attention\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ivirse/Data1/Project_ISOFH/Materials/NLP/InstructLLaMA/test.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m bsz, seqlen, _ \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/ivirse/Data1/Project_ISOFH/Materials/NLP/InstructLLaMA/test.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m xq, xk, xv \u001b[39m=\u001b[39m wq(x), wk(x), wv(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ivirse/Data1/Project_ISOFH/Materials/NLP/InstructLLaMA/test.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m xq \u001b[39m=\u001b[39m xq\u001b[39m.\u001b[39mview(bsz, seqlen, n_heads, head_dim)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ivirse/Data1/Project_ISOFH/Materials/NLP/InstructLLaMA/test.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m xk \u001b[39m=\u001b[39m xk\u001b[39m.\u001b[39mview(bsz, seqlen, n_heads, head_dim)\n",
      "File \u001b[0;32m~/anaconda3/envs/llma2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/llma2/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_tokens, terminal_steps in itertools.islice(train_loader, 64):\n",
    "    x = embedding(batch_tokens)\n",
    "    \n",
    "    # Attention\n",
    "    bsz, seqlen, _ = x.shape\n",
    "    xq, xk, xv = wq(x), wk(x), wv(x)\n",
    "\n",
    "    xq = xq.view(bsz, seqlen, n_heads, head_dim)\n",
    "    xk = xk.view(bsz, seqlen, n_heads, head_dim)\n",
    "    xv = xv.view(bsz, seqlen, n_heads, head_dim)\n",
    "    \n",
    "    keys = xk\n",
    "    values = xv\n",
    "    \n",
    "    xq = xq.transpose(1, 2)  # (bs, n_heads, seqlen, head_dim)\n",
    "    keys = keys.transpose(1, 2)\n",
    "    values = values.transpose(1, 2)\n",
    "    \n",
    "    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "    \n",
    "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "    \n",
    "    output = torch.matmul(scores, values)  # (bs, n_heads, seqlen, head_dim)\n",
    "    \n",
    "    output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "    output = wo(output) # bs, seqlen, dim\n",
    "    \n",
    "    # End attention\n",
    "    \n",
    "    # Head\n",
    "    output = scalar_head(output).float() # bs, seqlen, 1\n",
    "    outputs = output.squeeze(-1) # [num_combinations, = 2 seq_length]\n",
    "    \n",
    "    rewards = torch.gather(outputs, dim=1, index=terminal_steps.unsqueeze(-1)).squeeze(1)  # [num_combinations]\n",
    "    \n",
    "    print(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
